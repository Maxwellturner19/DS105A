{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style='font-size:3em'>üìù NB03 - WikiNews Scraping  </font>\n",
    "\n",
    "**PURPOSE**: This Jupyter Notebook contains scraping of wikinews articles on the elections from 2008-2020. There is also some analysis of the content of these articles. \n",
    "- There is no Election News on WikiNews before 2008 election\n",
    "\n",
    "**LAST REVISION:** 16th November 2023  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setting Up\n",
    "\n",
    "### Packages needed for this NB to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Search URl for WikiNews \n",
    "- set limit to 1200 to get the full search result and set offset to 0 to start from beginning \n",
    "- get the response and set my user-agent to my LSE email adress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_search_url = 'https://en.wikinews.org/w/index.php?title=Special:Search&limit=1200&offset=0&ns0=1&ns14=1&search=United+States+presidential+election'\n",
    "headers = {'User-Agent': 'm.filip-turner@lse.ac.uk'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create function to scrape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_search_results(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    sel = Selector(text=response.text)\n",
    "    return sel.css(\"div.mw-search-result-heading > a::attr(href)\").getall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create years variable for 1944-2024 going up by 4 \n",
    "- using this to filer the news articles for ones of interest\n",
    "- **I notice that only news articles for 2008-2020**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_results_by_years(links, years):\n",
    "    return [link for link in links if any(year in link for year in years)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create function to clean headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_headlines(headlines):\n",
    "    return [tag.replace('_', ' ').replace('%27', '').replace('/wiki/', '') for tag in headlines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create function to attatch base url to make links valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_base_url(links, base_url):\n",
    "    return [base_url + link for link in links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- apply the functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_search_result_headings = fetch_search_results(adv_search_url, headers)\n",
    "\n",
    "\n",
    "years = [str(year) for year in range(1944, 2025, 4)]\n",
    "filtered_news = filter_results_by_years(all_search_result_headings, years)\n",
    "\n",
    "base_url = \"https://en.wikinews.org\"\n",
    "news_urls = add_base_url(filtered_news, base_url)\n",
    "\n",
    "filtered_news = clean_headlines(filtered_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames for news headlines and links\n",
    "\n",
    "### + Saving these DFs as CSVs as \"df_news.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.DataFrame({'News URL': news_urls , 'Headlines': filtered_news})\n",
    "df_news.to_csv('Data/df_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scrape the text from each of the news articles for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers)\n",
    "        sel = Selector(text=response.text)\n",
    "        text = sel.css(\"div.mw-parser-output > p::text\").getall()\n",
    "        return ' '.join(text)\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return ''  # Return an empty string in case of an error\n",
    "\n",
    "def get_articles_with_year(df, year):\n",
    "    filtered_urls = df[df['News URL'].str.contains(str(year))]['News URL']\n",
    "    return [extract_text_from_url(url) for url in filtered_urls]\n",
    "\n",
    "# Load your DataFrame\n",
    "df_news = pd.read_csv('Data/df_news.csv')\n",
    "\n",
    "# Use the function to get articles for the year 2008\n",
    "year_2008_article_text = get_articles_with_year(df_news, 2008)\n",
    "year_2012_article_text = get_articles_with_year(df_news, 2012)\n",
    "year_2016_article_text = get_articles_with_year(df_news, 2016)\n",
    "year_2020_article_text = get_articles_with_year(df_news, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some text analysis of the news articles \n",
    "\n",
    "### + Create wordle the news articles relevent to each election and export them as png to /Data folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def generate_wordcloud_for_year(df, year, additional_stopwords=None, save_to_file=False):\n",
    "    if additional_stopwords is None:\n",
    "        additional_stopwords = []\n",
    "\n",
    "    # Filter and extract text for the given year\n",
    "    year_text = get_articles_with_year(df, year)\n",
    "\n",
    "    # Combine all texts and preprocess\n",
    "    combined_text = ' '.join(year_text).lower()\n",
    "    words = combined_text.split()\n",
    "\n",
    "    # Filter stopwords\n",
    "    all_stopwords = stopwords.words('english') + ['said', 'would', 'also', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten'] + additional_stopwords\n",
    "    filtered_words = [word for word in words if word not in all_stopwords]\n",
    "\n",
    "    # Generate word cloud\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    wordcloud = WordCloud(width=800, height=800, background_color='white').generate(filtered_text)\n",
    "\n",
    "    # Save or display word cloud\n",
    "    if save_to_file:\n",
    "        plt.figure(figsize=(8, 8), facecolor=None)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.savefig(f'Data/wordcloud_{year}.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.figure(figsize=(8, 8), facecolor=None)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "df_news = pd.read_csv('Data/df_news.csv')\n",
    "for year in [2008, 2012, 2016, 2020]:\n",
    "    generate_wordcloud_for_year(df_news, year, save_to_file=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñ Takeaways from wordle\n",
    "\n",
    "- The wordle gives brief insight to the key people and topics of each election.\n",
    "- In the 2008 election we see the word **Police** come up as wuite prominent which would be a good indicator of potential domestic unrest as a key theme for the election\n",
    "- Similarly, 2020 we see **war** show up which hints of the impact of Russia and Ukraine on the election"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### + Calculate an average polarity score for articles relevent to each election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_polarity_for_year(df, year):\n",
    "    # Filter and extract text for the given year\n",
    "    year_text = get_articles_with_year(df, year)\n",
    "\n",
    "    # Combine all texts for each article\n",
    "    articles = [' '.join(article) for article in year_text]\n",
    "\n",
    "    # Calculate polarity scores for each article\n",
    "    polarity_scores = [TextBlob(article).sentiment.polarity for article in articles]\n",
    "\n",
    "    # Compute average polarity score\n",
    "    if polarity_scores:\n",
    "        average_polarity = sum(polarity_scores) / len(polarity_scores)\n",
    "    else:\n",
    "        average_polarity = 0  # Default value in case there are no articles\n",
    "\n",
    "    return average_polarity\n",
    "\n",
    "# Example usage\n",
    "df_news = pd.read_csv('Data/df_news.csv')\n",
    "\n",
    "years = [2008, 2012, 2016, 2020]\n",
    "average_polarities = {}\n",
    "\n",
    "for year in years:\n",
    "    average_polarity = calculate_average_polarity_for_year(df_news, year)\n",
    "    average_polarities[year] = average_polarity\n",
    "\n",
    "# Optional: Save the average polarities to a CSV file\n",
    "df_average_polarities = pd.DataFrame(list(average_polarities.items()), columns=['Year', 'Average Polarity'])\n",
    "df_average_polarities.to_csv('Data/average_polarity_by_year.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñ Takeaways from Polarity measure\n",
    "\n",
    "- 2008: The average polarity score of 0.10625 suggests a slightly positive sentiment in news articles for this election year.\n",
    "- 2012: This year shows an average polarity score of -0.00864, indicating a nearly neutral but slightly negative sentiment in the news coverage.\n",
    "- 2016: The average score of 0.0625 again indicates a slightly positive sentiment, though not as strong as in 2008.\n",
    "- 2020: With an average polarity score of 0.03385, the sentiment in news articles was slightly positive, but closer to neutral compared to other years.\n",
    "\n",
    "- The WikiNews sources are often very neutral news peices relaying information and little opinion or emotive language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I want to merge the polarity measure for election news artiles I have to the Elections_table_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_election = pd.read_csv('Data/Election_table_data.csv')\n",
    "merged_df = pd.merge(df_election, df_average_polarities, on='Year', how='left')\n",
    "merged_df.to_csv('Data/Elections_polarity_merged_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PURPOSE**: This Jupyter Notebook contains scraping of wikinews articles on the elections from 2008-2020. There is also some analysis of the content of these articles. \n",
    "\n",
    "- The purpose of WikiNews is to back up the polarity of elections with analysis of the news articles relating to elections.\n",
    "- Higher polaroty score would indicate that it is a close election and turnout should be higher. \n",
    "- If news articles were dates back to 1944 on WIkinews, this analysis would have been more informative. i was however limited my the availability of News articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
