{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style='font-size:3em'>**üìù DS105 Week 7 Summative Assessment** </font>\n",
    "\n",
    "**PURPOSE**: This Jupyter Notebook contains the scraping of the page titles and links for the Wikipedia pages with the US presidential Elections from 1944-2024. I create a CSV file with the URLs of each 21 pages and I create a dataframe with this information called *Elections.cvs*\n",
    "\n",
    "<a href=\"https://en.wikipedia.org/w/index.php?title=Special:Search&limit=56201&offset=0&ns0=1&search=United+States+presidential+election\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Wikipedia-logo-v2-en.svg/300px-Wikipedia-logo-v2-en.svg.png\" alt=\"Wikipedia\" width=\"10%\">\n",
    "</a>\n",
    "\n",
    "**LAST REVISION:** *16th November 2023*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setting Up\n",
    "\n",
    "### Packages needed for this NB to run  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from scrapy import Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I set the limit to 56201 to get all results and set offset to 0 to start from the beginning\n",
    "- This will maximise the scope of my scrape and the results\n",
    "- I set the User-agent to my LSE Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_search_url = 'https://en.wikipedia.org/w/index.php?title=Special:Search&limit=56201&offset=0&ns0=1&search=United+States+presidential+election'\n",
    "headers = {'User-Agent': 'm.filip-turner@lse.ac.uk'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I create function for getting search results from advanced search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_results(url, headers):\n",
    "    response = requests.get(adv_search_url, headers=headers)\n",
    "    sel = Selector(text=response.text)\n",
    "    return sel.css(\"div.mw-search-result-heading > a::attr(href)\").getall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I create function for cleaning data by removing '/wiki/' and '_' from title headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(title):\n",
    "    \"\"\"Clean and format the title string.\"\"\"\n",
    "    return title.replace(\"/wiki/\", \"\").replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I will then filter the titles to contain only 1944-2024 and take those out that do not using function\n",
    "- I noticed that the results gave me headings for 50 state elections accross the 21 years. Therefore 1050 headings I did not want plus a 2 standalone headings containing predictions and recounts. So I got rid of these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_elections(titles, start_year=1944, end_year=2024):\n",
    "    \"\"\"Filter election titles based on given criteria.\"\"\"\n",
    "    elections = []\n",
    "    for title in titles:\n",
    "        if \"United States presidential election\" in title:\n",
    "            year = title.split(' ')[0]\n",
    "            try:\n",
    "                if start_year <= int(year) <= end_year:\n",
    "                    elections.append(title)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return [election for election in sorted(elections, reverse=True) \n",
    "            if \"in\" not in election \n",
    "            and election not in [\"2020 United States presidential election predictions\", \n",
    "                                 \"2016 United States presidential election recounts\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I create a function for getting the URl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_election_urls(titles, base_url):\n",
    "    urls = []\n",
    "    for title in titles:\n",
    "        title_for_url = title.replace(' ', '_')\n",
    "        params = {'action': 'query', 'format': 'json', 'titles': title_for_url}\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "        page_id = next(iter(data['query']['pages']))\n",
    "        if 'missing' not in data['query']['pages'][page_id]:\n",
    "            page_title = data['query']['pages'][page_id]['title']\n",
    "            page_url = f\"https://en.wikipedia.org/wiki/{page_title.replace(' ', '_')}\"\n",
    "            urls.append(page_url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I use list comprehensions to parse all URLs and apply the  filter_elections function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all search result headings\n",
    "all_search_result_headings = get_search_results(adv_search_url, headers)\n",
    "all_search_result_headings = [clean_title(title) for title in all_search_result_headings]\n",
    "\n",
    "# Filter the election results\n",
    "filtered_elections = filter_elections(all_search_result_headings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URLs for filtered elections\n",
    "api_base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "election_urls = get_election_urls(filtered_elections, api_base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create DF and export to /Data file as CSV\n",
    "- Saved this as *\"Elections.csv\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elections = pd.DataFrame({'Link': election_urls, 'Elections': filtered_elections})\n",
    "df_elections.to_csv('Data/Elections.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PURPOSE**: This Jupyter Notebook contains the scraping of the page titles and links for the Wikipedia pages with the US presidential Elections from 1944-2024. I create a CSV file with the URLs of each 21 pages and I create a dataframe with this information called *Elections.cvs*\n",
    "\n",
    "- The dataframe created from this NB allows me to use the URLs for each election article to scrape through the results table in each and create a more informative df for my analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
